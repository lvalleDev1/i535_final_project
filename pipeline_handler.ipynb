{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a75d1a74-9db5-4daa-baf0-6494d7f589b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# authenticate\n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "SUBSCRIPTION=\"bc3b3ce8-f4bb-4520-b098-ab59eb6b957e\"\n",
    "RESOURCE_GROUP=\"lv.developer-rg\"\n",
    "WS_NAME=\"iu_workspace\"\n",
    "# get a handle to the workspace\n",
    "ml_client = MLClient(\n",
    "    credential=credential,\n",
    "    subscription_id=SUBSCRIPTION,\n",
    "    resource_group_name=RESOURCE_GROUP,\n",
    "    workspace_name=WS_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ff25d60-a6d9-4662-9a81-b6be040e56ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eastus2 : lv.developer-rg\n"
     ]
    }
   ],
   "source": [
    "# verify that the handle works correctly\n",
    "ws = ml_client.workspaces.get(WS_NAME)\n",
    "print(ws.location,\":\", ws.resource_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "790ee804-8d18-47a7-8672-05f07d97fee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data asset URI: azureml://subscriptions/bc3b3ce8-f4bb-4520-b098-ab59eb6b957e/resourcegroups/lv.developer-rg/workspaces/iu_workspace/datastores/workspaceblobstore/paths/LocalUpload/6663458963178a7f00c5dfcc8f95824b/Crimes_-_2001_to_Present.csv\n"
     ]
    }
   ],
   "source": [
    "# get a handle of the data asset and print URI\n",
    "chicago_crime_data = ml_client.data.get(name=\"chicago-crime\", version=\"initial\")\n",
    "print(f\"Data asset URI: {chicago_crime_data.path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80e32a5b-7a59-4937-9dcb-4dd18e43e7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dependencies_dir = \"./dependencies\"\n",
    "os.makedirs(dependencies_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b7b93db-19bc-4009-b7c6-a276ef11dd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./dependencies/conda.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {dependencies_dir}/conda.yaml\n",
    "name: model-env\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.8\n",
    "  - numpy=1.21.2\n",
    "  - pip=21.2.4\n",
    "  - scikit-learn=0.24.2\n",
    "  - scipy=1.7.1\n",
    "  - pandas>=1.1,<1.2\n",
    "  - pip:\n",
    "    - inference-schema[numpy-support]==1.3.0\n",
    "    - xlrd==2.0.1\n",
    "    - mlflow== 2.4.1\n",
    "    - azureml-mlflow==1.51.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c2f2e50-7fea-4833-b90f-c8e699bab21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment with name aml-scikit-learn is registered to workspace, the environment version is 0.2.0\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import Environment\n",
    "\n",
    "custom_env_name = \"aml-scikit-learn\"\n",
    "\n",
    "pipeline_job_env = Environment(\n",
    "    name=custom_env_name,\n",
    "    description=\"Custom environment for Credit Card Defaults pipeline\",\n",
    "    tags={\"scikit-learn\": \"0.24.2\"},\n",
    "    conda_file=os.path.join(dependencies_dir, \"conda.yaml\"),\n",
    "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
    "    version=\"0.2.0\",\n",
    ")\n",
    "pipeline_job_env = ml_client.environments.create_or_update(pipeline_job_env)\n",
    "\n",
    "print(\n",
    "    f\"Environment with name {pipeline_job_env.name} is registered to workspace, the environment version is {pipeline_job_env.version}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87f934a1-3e71-4699-b093-2bab3269f9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_preprocessing_src_dir = \"./components/data_preprocessing\"\n",
    "os.makedirs(data_preprocessing_src_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "537ab747-2f4c-4032-8f65-f7bc7bc0941c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./components/data_preprocessing/data_preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {data_preprocessing_src_dir}/data_preprocessing.py\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import logging\n",
    "import mlflow\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function of the script.\"\"\"\n",
    "\n",
    "    # input and output arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--raw_data\", type=str, help=\"path to input data\")\n",
    "    parser.add_argument(\"--preprocessed_data\", type=str, help=\"path to output data\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # start Logging\n",
    "    mlflow.start_run()\n",
    "\n",
    "    print(\" \".join(f\"{k}={v}\" for k, v in vars(args).items()))\n",
    "\n",
    "    print(\"input data:\", args.raw_data)\n",
    "\n",
    "    crime_df = pd.read_csv(args.raw_data)\n",
    "    \n",
    "    mlflow.log_metric(\"num_samples_original_data:\", crime_df.shape[0])\n",
    "    mlflow.log_metric(\"num_features_original_data\", crime_df.shape[1])\n",
    "    \n",
    "    crime_df.drop(\n",
    "        [\n",
    "            'Case Number',\n",
    "            'Ward',\n",
    "            'Community Area',\n",
    "            'District',\n",
    "            'Location Description',\n",
    "            'Description',\n",
    "            'Location',\n",
    "            'Block',\n",
    "            'IUCR',\n",
    "            'Beat',\n",
    "            'FBI Code',\n",
    "            'ID'\n",
    "        ],\n",
    "        axis=1,\n",
    "        inplace=True\n",
    "    )\n",
    "    crime_df = crime_df[~crime_df['X Coordinate'].isna()]\n",
    "    \n",
    "    mlflow.log_metric(\"num_samples_preprocessed_data:\", crime_df.shape[0])\n",
    "    mlflow.log_metric(\"num_features_preprocessed_data\", crime_df.shape[1])\n",
    "\n",
    "    # output path mounted as folder -> add filename to the path\n",
    "    crime_df.to_csv(os.path.join(args.preprocessed_data, \"crime_data_preprocessed.csv\"), index=False)\n",
    "\n",
    "    # stop Logging\n",
    "    mlflow.end_run()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5eec566-38ad-45f9-bd73-a8fcf4396be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml import Input, Output\n",
    "\n",
    "data_preprocessing_component = command(\n",
    "    name=\"data_preprocessing_chicago_crime\",\n",
    "    display_name=\"Data preprocessing\",\n",
    "    description=\"remove unecessary columns\",\n",
    "    inputs={\n",
    "        \"raw_data\": Input(type=\"uri_folder\"),\n",
    "    },\n",
    "    outputs=dict(\n",
    "        preprocessed_data=Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "    ),\n",
    "    # source folder of the component\n",
    "    code=data_preprocessing_src_dir,\n",
    "    command=\"\"\"python data_preprocessing.py \\\n",
    "            --raw_data ${{inputs.raw_data}} \\\n",
    "            --preprocessed_data ${{outputs.preprocessed_data}} \\\n",
    "            \"\"\",\n",
    "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96c70330-84ab-424c-b0df-02182b7b197e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading data_preprocessing (0.0 MBs): 100%|██████████| 3125/3125 [00:00<00:00, 42482.60it/s]\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component data_preprocessing_chicago_crime with Version 2024-04-30-19-59-40-8716971 is registered\n"
     ]
    }
   ],
   "source": [
    "# register the component to the workspace\n",
    "data_preprocessing_component = ml_client.create_or_update(data_preprocessing_component.component)\n",
    "\n",
    "print(\n",
    "    f\"Component {data_preprocessing_component.name} with Version {data_preprocessing_component.version} is registered\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff67ddf6-b93b-4830-aba4-c4fa0c5ed46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "feature_engineering_src_dir = \"./components/feature_engineering\"\n",
    "os.makedirs(feature_engineering_src_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e01ca04c-0fb3-4c72-b697-17145e0f4472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./components/feature_engineering/feature_engineering.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {feature_engineering_src_dir}/feature_engineering.py\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "import mlflow\n",
    "\n",
    "def select_first_file(path):\n",
    "    \"\"\"Selects first file in folder, use under assumption there is only one file in folder\n",
    "    Args:\n",
    "        path (str): path to directory or file to choose\n",
    "    Returns:\n",
    "        str: full path of selected file\n",
    "    \"\"\"\n",
    "    files = os.listdir(path)\n",
    "    return os.path.join(path, files[0])\n",
    "\n",
    "\n",
    "# start Logging\n",
    "mlflow.start_run()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function of the script.\"\"\"\n",
    "\n",
    "    # input and output arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--preprocessed_data\", type=str, help=\"path to input data\")\n",
    "    parser.add_argument(\"--train_data\", type=str, help=\"path to train data\")\n",
    "    parser.add_argument(\"--test_data\", type=str, help=\"path to test data\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print(\" \".join(f\"{k}={v}\" for k, v in vars(args).items()))\n",
    "\n",
    "    print(\"input data:\", args.preprocessed_data)\n",
    "    \n",
    "    crime_df = pd.read_csv(select_first_file(args.preprocessed_data))\n",
    "    \n",
    "    print(f\"log 1 - crime_df.shape: {crime_df.shape}\")\n",
    "    print(f\"log 2 - crime_df.columns: {crime_df.columns}\")\n",
    "\n",
    "    mlflow.log_metric(\"num_samples_preprocessed_data:\", crime_df.shape[0])\n",
    "    mlflow.log_metric(\"num_features_preprocessed_data\", crime_df.shape[1])\n",
    "    \n",
    "    crime_df['Date'] = crime_df['Date'].str[0:10]\n",
    "    crime_df['Datetime'] = pd.to_datetime(crime_df['Date'], format='%m/%d/%Y')\n",
    "    crime_df['Day Name'] = crime_df['Datetime'].dt.day_name()\n",
    "    \n",
    "    crime_df['Day'] = crime_df['Date'].str[3:5]\n",
    "    crime_df['Month'] = crime_df['Date'].str[0:2]\n",
    "    \n",
    "    crime_df['Updated On Day'] = crime_df['Updated On'].str[3:5]\n",
    "    crime_df['Updated On Month'] = crime_df['Updated On'].str[0:2]\n",
    "    crime_df['Updated On Year'] = crime_df['Updated On'].str[6:10]\n",
    "    \n",
    "    crime_df.drop(\n",
    "        [\n",
    "            'Date',\n",
    "            'Updated On',\n",
    "            'Datetime',\n",
    "        ],\n",
    "        axis=1,\n",
    "        inplace=True\n",
    "    )\n",
    "    \n",
    "    mlflow.log_metric(\"num_samples_feature_engineered_data:\", crime_df.shape[0])\n",
    "    mlflow.log_metric(\"num_features_feature_engineered_data\", crime_df.shape[1])\n",
    "    \n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    label_encoder.fit(crime_df['Primary Type'])\n",
    "    crime_df['Primary Type Cat'] = label_encoder.transform(crime_df['Primary Type'])\n",
    "    label_encoder.fit(crime_df['Arrest'])\n",
    "    crime_df['Arrest Cat'] = label_encoder.transform(crime_df['Arrest'])\n",
    "    label_encoder.fit(crime_df['Domestic'])\n",
    "    crime_df['Domestic Cat'] = label_encoder.transform(crime_df['Domestic'])\n",
    "    label_encoder.fit(crime_df['Day Name'])\n",
    "    crime_df['Day Name Cat'] = label_encoder.transform(crime_df['Day Name'])\n",
    "    \n",
    "    df_hotencoded = crime_df[\n",
    "        [\n",
    "            'Primary Type Cat',\n",
    "            'Domestic Cat',\n",
    "            'Day Name Cat',\n",
    "            'Day',\n",
    "            'Month',\n",
    "            'Year',\n",
    "            'Updated On Day',\n",
    "            'Updated On Month',\n",
    "            'Updated On Year',\n",
    "            'X Coordinate',\n",
    "            'Y Coordinate',\n",
    "            'Latitude',\n",
    "            'Longitude',\n",
    "            'Arrest Cat'\n",
    "        ]\n",
    "    ]\n",
    "    \n",
    "    mlflow.log_metric(\"num_samples_hotencoded_data:\", crime_df.shape[0])\n",
    "    mlflow.log_metric(\"num_features_hotencoded_data\", crime_df.shape[1])\n",
    "    \n",
    "    train_df, test_df = train_test_split(\n",
    "        df_hotencoded,\n",
    "        test_size=0.2,\n",
    "    )\n",
    "\n",
    "    # output paths mounted as folder -> add filename to the path\n",
    "    train_df.to_csv(os.path.join(args.train_data, \"training_data.csv\"), index=False)\n",
    "    test_df.to_csv(os.path.join(args.test_data, \"testing_data.csv\"), index=False)\n",
    "\n",
    "    # Stop Logging\n",
    "    mlflow.end_run()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7ead7ce-4113-4971-ad45-8847c332a921",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml import Input, Output\n",
    "\n",
    "feature_engineering_component = command(\n",
    "    name=\"feature_engineering_chicago_crime\",\n",
    "    display_name=\"Feature engineering\",\n",
    "    description=\"create new relevant attributes\",\n",
    "    inputs={\n",
    "        \"preprocessed_data\": Input(type=\"uri_folder\"),\n",
    "    },\n",
    "    outputs=dict(\n",
    "        train_data=Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "        test_data=Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "    ),\n",
    "    # source folder of the component\n",
    "    code=feature_engineering_src_dir,\n",
    "    command=\"\"\"python feature_engineering.py \\\n",
    "            --preprocessed_data ${{inputs.preprocessed_data}} \\\n",
    "            --train_data ${{outputs.train_data}} --test_data ${{outputs.test_data}} \\\n",
    "            \"\"\",\n",
    "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1099572f-8a84-4480-a98b-96bf44972658",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading feature_engineering (0.01 MBs): 100%|██████████| 5584/5584 [00:00<00:00, 74088.00it/s]\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component feature_engineering_chicago_crime with Version 2024-04-30-19-59-43-0964239 is registered\n"
     ]
    }
   ],
   "source": [
    "# register the component to the workspace\n",
    "feature_engineering_component = ml_client.create_or_update(feature_engineering_component.component)\n",
    "\n",
    "print(\n",
    "    f\"Component {feature_engineering_component.name} with Version {feature_engineering_component.version} is registered\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42edae22-3981-4df8-a813-2205b52199cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "train_src_dir = \"./components/train\"\n",
    "os.makedirs(train_src_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "429ef2e1-b94d-4af6-bf72-a38432133820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./components/train/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {train_src_dir}/train.py\n",
    "import argparse\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "\n",
    "\n",
    "def select_first_file(path):\n",
    "    \"\"\"Selects first file in folder, use under assumption there is only one file in folder\n",
    "    Args:\n",
    "        path (str): path to directory or file to choose\n",
    "    Returns:\n",
    "        str: full path of selected file\n",
    "    \"\"\"\n",
    "    files = os.listdir(path)\n",
    "    return os.path.join(path, files[0])\n",
    "\n",
    "\n",
    "# start Logging\n",
    "mlflow.start_run()\n",
    "\n",
    "# enable autologging\n",
    "mlflow.sklearn.autolog()\n",
    "\n",
    "os.makedirs(\"./outputs\", exist_ok=True)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function of the script.\"\"\"\n",
    "\n",
    "    # input and output arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--train_data\", type=str, help=\"path to train data\")\n",
    "    parser.add_argument(\"--test_data\", type=str, help=\"path to test data\")\n",
    "    parser.add_argument(\"--registered_model_name\", type=str, help=\"model name\")\n",
    "    parser.add_argument(\"--model\", type=str, help=\"path to model file\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # paths mounted as folder -> select file from folder\n",
    "    train_df = pd.read_csv(select_first_file(args.train_data))\n",
    "    test_df = pd.read_csv(select_first_file(args.test_data))\n",
    "\n",
    "    X_train = train_df[\n",
    "        [\n",
    "            'Primary Type Cat',\n",
    "            'Domestic Cat',\n",
    "            'Day Name Cat',\n",
    "            'Day',\n",
    "            'Month',\n",
    "            'Year',\n",
    "            'Updated On Day',\n",
    "            'Updated On Month',\n",
    "            'Updated On Year',\n",
    "            'X Coordinate',\n",
    "            'Y Coordinate',\n",
    "            'Latitude',\n",
    "            'Longitude',\n",
    "        ]\n",
    "    ].to_numpy()\n",
    "    \n",
    "    y_train = train_df[\n",
    "        [\n",
    "           'Arrest Cat' \n",
    "        ]\n",
    "    ].to_numpy()\n",
    "    \n",
    "    X_test = test_df[\n",
    "        [\n",
    "           'Primary Type Cat',\n",
    "            'Domestic Cat',\n",
    "            'Day Name Cat',\n",
    "            'Day',\n",
    "            'Month',\n",
    "            'Year',\n",
    "            'Updated On Day',\n",
    "            'Updated On Month',\n",
    "            'Updated On Year',\n",
    "            'X Coordinate',\n",
    "            'Y Coordinate',\n",
    "            'Latitude',\n",
    "            'Longitude',\n",
    "        ]\n",
    "    ].to_numpy()\n",
    "\n",
    "    y_test = test_df[\n",
    "        [\n",
    "           'Arrest Cat' \n",
    "        ]\n",
    "    ].to_numpy()\n",
    "    \n",
    "    y_train = y_train.ravel()\n",
    "    y_test = y_test.ravel()\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    \n",
    "    svm = LinearSVC(C=0.0001)\n",
    "    svm.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = svm.predict(X_test)\n",
    "\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # registering the model to the workspace\n",
    "    print(\"Registering the model via MLFlow\")\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=svm,\n",
    "        registered_model_name=args.registered_model_name,\n",
    "        artifact_path=args.registered_model_name,\n",
    "    )\n",
    "\n",
    "    # saving the model to a file\n",
    "    mlflow.sklearn.save_model(\n",
    "        sk_model=svm,\n",
    "        path=os.path.join(args.model, \"trained_model\"),\n",
    "    )\n",
    "\n",
    "    # stop Logging\n",
    "    mlflow.end_run()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1b0ceeb-5e0e-460a-9f0a-0b1334e97e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./components/train/train.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {train_src_dir}/train.yml\n",
    "# <component>\n",
    "name: train_chicago_crime_model\n",
    "display_name: Train Chicago Crime Model\n",
    "# version: 1 # Not specifying a version will automatically update the version\n",
    "type: command\n",
    "inputs:\n",
    "  train_data: \n",
    "    type: uri_folder\n",
    "  test_data: \n",
    "    type: uri_folder    \n",
    "  registered_model_name:\n",
    "    type: string\n",
    "outputs:\n",
    "  model:\n",
    "    type: uri_folder\n",
    "code: .\n",
    "environment:\n",
    "  # use an AzureML curate environment\n",
    "  azureml:AzureML-sklearn-1.0-ubuntu20.04-py38-cpu:1\n",
    "command: >-\n",
    "  python train.py \n",
    "  --train_data ${{inputs.train_data}} \n",
    "  --test_data ${{inputs.test_data}} \n",
    "  --registered_model_name ${{inputs.registered_model_name}} \n",
    "  --model ${{outputs.model}}\n",
    "# </component>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc158d05-401d-419f-9177-9e80e1f86315",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading train (0.0 MBs): 100%|██████████| 3760/3760 [00:00<00:00, 60618.32it/s]\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component train_chicago_crime_model with Version 2024-04-30-19-59-45-3124006 is registered\n"
     ]
    }
   ],
   "source": [
    "# importing the Component Package\n",
    "from azure.ai.ml import load_component\n",
    "\n",
    "# loading the component from the yml file\n",
    "train_component = load_component(source=os.path.join(train_src_dir, \"train.yml\"))\n",
    "\n",
    "# register component to the workspace\n",
    "train_component = ml_client.create_or_update(train_component)\n",
    "\n",
    "print(\n",
    "    f\"Component {train_component.name} with Version {train_component.version} is registered\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68fb7a34-ff8a-4dc2-89f2-8422ce005504",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import dsl, Input, Output\n",
    "\n",
    "# dsl decorator tells the sdk that we are defining an Azure Machine Learning pipeline\n",
    "@dsl.pipeline(\n",
    "    compute=\"serverless\",  # \"serverless\" value runs pipeline on serverless compute\n",
    "    description=\"E2E data_perp-train pipeline\",\n",
    ")\n",
    "def crime_chicago_pipeline(\n",
    "    pipeline_job_data_preprocessing_input,\n",
    "    pipeline_job_registered_model_name,\n",
    "):\n",
    "    # using data_preprocessing_function like a python call with its own inputs\n",
    "    data_preprocessing_job = data_preprocessing_component(\n",
    "        raw_data=pipeline_job_data_preprocessing_input,\n",
    "    )\n",
    "    \n",
    "    # using feature_engineering_function like a python call with its own inputs and reading output of previous component\n",
    "    feature_engineering_job = feature_engineering_component(\n",
    "        preprocessed_data=data_preprocessing_job.outputs.preprocessed_data, # note: using outputs from previous step\n",
    "    )\n",
    "\n",
    "    # using train_func like a python call with its own inputs and reading output of previous component\n",
    "    train_job = train_component(\n",
    "        train_data=feature_engineering_job.outputs.train_data,  # note: using outputs from previous step\n",
    "        test_data=feature_engineering_job.outputs.test_data,  # note: using outputs from previous step\n",
    "        registered_model_name=pipeline_job_registered_model_name,\n",
    "    )\n",
    "\n",
    "    # pipeline returns a dictionary of outputs\n",
    "    # keys will code for the pipeline output identifier\n",
    "    return {\n",
    "        \"pipeline_job_train_data\": feature_engineering_job.outputs.train_data,\n",
    "        \"pipeline_job_test_data\": feature_engineering_job.outputs.test_data,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "401432c6-1cd7-472e-ad1c-9880d8447d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "registered_model_name = \"chicago_crime_model\"\n",
    "\n",
    "# Let's instantiate the pipeline with the parameters of our choice\n",
    "pipeline = crime_chicago_pipeline(\n",
    "    pipeline_job_data_preprocessing_input=Input(type=\"uri_file\", path=chicago_crime_data.path),\n",
    "    pipeline_job_registered_model_name=registered_model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e158834-5145-4a63-8242-ea505a22ed75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class AutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class AutoDeleteConditionSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseAutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class IntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class ProtectionLevelSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseIntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: gray_head_nq24vc4513\n",
      "Web View: https://ml.azure.com/runs/gray_head_nq24vc4513?wsid=/subscriptions/bc3b3ce8-f4bb-4520-b098-ab59eb6b957e/resourcegroups/lv.developer-rg/workspaces/iu_workspace\n",
      "\n",
      "Streaming logs/azureml/executionlogs.txt\n",
      "========================================\n",
      "\n",
      "[2024-04-30 19:59:50Z] Submitting 1 runs, first five are: 8d1b49c4:6026b606-7296-4f09-a452-9fc3e4d8db61\n",
      "[2024-04-30 20:05:27Z] Completing processing run id 6026b606-7296-4f09-a452-9fc3e4d8db61.\n",
      "[2024-04-30 20:05:27Z] Submitting 1 runs, first five are: 428442d0:e84c798b-f811-4222-9500-2000593054ef\n",
      "[2024-04-30 20:12:09Z] Completing processing run id e84c798b-f811-4222-9500-2000593054ef.\n",
      "[2024-04-30 20:12:09Z] Submitting 1 runs, first five are: d7962a9c:92703662-a09d-41b8-ae28-4e7f5a53a463\n",
      "[2024-04-30 20:15:45Z] Completing processing run id 92703662-a09d-41b8-ae28-4e7f5a53a463.\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: gray_head_nq24vc4513\n",
      "Web View: https://ml.azure.com/runs/gray_head_nq24vc4513?wsid=/subscriptions/bc3b3ce8-f4bb-4520-b098-ab59eb6b957e/resourcegroups/lv.developer-rg/workspaces/iu_workspace\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# submit the pipeline job\n",
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline,\n",
    "    # Project's name\n",
    "    experiment_name=\"e2e_registered_components\",\n",
    ")\n",
    "ml_client.jobs.stream(pipeline_job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9dd63f-646b-43ab-8d5f-1d9e9cde8c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67e6d6b-ba0a-4db8-a1ff-d9bb3e58fe7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 - SDK v2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
